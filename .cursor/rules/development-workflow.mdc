---
alwaysApply: true
---

# Development Workflow

This rule describes the step-by-step workflow for implementing features and fixes in the Spark project.

## Purpose

**This workflow is designed to maximize autonomy and minimize human interaction, speeding up development.**

Key principles:
- **Minimize token usage** – No over-verbose explanations
- **Track your progress** – Use TODO lists when going through the plan
- **Keep plans concise** – Don't write actual code implementation into the plan, only high-level approach
- **Self-validate** – Verify changes yourself before asking for human review
- **Be autonomous** – Work through all steps independently when possible

## Workflow Steps

### 1. Plan
- Understand the requirements and scope
- Review relevant context, specs, and existing code
- Identify affected components and dependencies
- **Create TODO list** to track progress through the workflow
- **Keep it high-level** – no actual code implementation in the plan

### 2. Decision
- Figure out what options we have
- Identify open questions regarding:
  - Implementation approach
  - Desired functionality
  - Architecture considerations
  - Design patterns to use

#### 2b. Baseline Check
Run `npm run check` to note the current state:
- Linting status
- Formatting status
- Code coverage baseline

This establishes a reference point before making changes.

### 3. Implementation
- Implement the feature or fix
- **Don't overengineer** – keep it minimal and elegant
- **Respect existing design patterns** or use new suitable ones
- Don't duplicate code, inspect related modules and methods before adding new ones
- **Don't just add more code** – minimize and refine
- **Minimize workarounds** – stick to robust implementations
- Follow established patterns from the codebase

### 4. Check for Errors
Run `npm run check` (works for both daemon and plugin):
- For daemon: runs tests with coverage
- For plugin: runs linting and formatting checks

### 5. Address Issues
- Fix any issues found in the check step
- If no issues found, proceed to the next step
- Re-run check after fixes to ensure they're resolved

### 6. Check Coverage
If coverage decreased:

**a) Check if we added new code that's not covered:**
- Add tests for the new code
- Run tests and check coverage again

**b) Check if existing code is no longer executed:**
- May happen due to refactoring, new methods, logic changes
- Options:
  - Clean up old methods that are no longer used
  - Rethink if the new methods are better
  - Ensure logic changes are intentional and complete

**c) Final validation:**
- After all fixes and refactors, run tests one final time
- Ensure coverage is maintained or improved
- Proceed to next step only when coverage is acceptable

### 7. Hot-Reload Verification
- The project has hot-reloader for both frontend and backend
- In most cases, it should pick up changes automatically
- If in doubt:
  - Rebuild manually
  - Or suggest a fix to the hot-reloader

### 8. Self-Validation

**For Daemon:**
- Use console logs and test output
- Run daemon in debug mode to see detailed logging
- Check test coverage and test output

**For Plugin (98% Autonomous with MCP):**
- **Playwright MCP enables autonomous validation**
- Connect to Obsidian via remote debugging (port 9222)
- Type keyboard commands (`/`, `@`, `Cmd+K`) automatically
- Read console logs automatically
- Capture screenshots for verification
- Inspect DOM elements programmatically
- Monitor network requests to daemon
- **Only ask user if truly unable to verify**

**MCP Validation Checklist:**
1. Ensure Obsidian is running with: `open -a Obsidian --args --remote-debugging-port=9222`
2. Connect via MCP (available after session restart)
3. Type commands to trigger features
4. Read console logs for output
5. Capture screenshots for visual confirmation
6. Verify DOM state (elements exist, classes correct)
7. Check for JavaScript errors
8. Iterate until working

**Best Practices:**
- Add strategic console.log statements: `console.log('[Spark Debug] Feature triggered:', data)`
- Use semantic class names: `spark-palette`, `spark-chat`
- Add data attributes for testing: `data-spark-element="palette"`
- Store logs globally: `window.sparkDebugLogs = []` for MCP access

### 9. User Feedback
- If unable to verify that the plugin works, ask the user
- User will check and provide feedback
- Based on feedback:
  - Return to step 3 (Implementation)
  - Apply fixes
  - Go through steps 4-9 again

## Key Commands

**Daemon:**
- `npm run check` – format, lint, type-check, tests with coverage
- `npm run dev` – development mode with auto-restart
- `npm run dev:debug` – development mode with debug logging

**Plugin:**
- `npm run check` – format, lint, type-check
- `npm run dev` – development mode with hot-reload
- Reload in Obsidian: `Cmd+R`

**MCP Validation:**
- Launch Obsidian: `open -a Obsidian --args --remote-debugging-port=9222`
- Verify connection: `curl http://localhost:9222/json`
- Check MCP config: `cat ~/.claude.json | grep playwright`

## Plugin Validation with Playwright MCP

**Status:** ✅ Configured and Ready (see DEVELOPER_EXPERIENCE.md for details)

**Setup (one-time):**
```bash
claude mcp add playwright -s user -- npx -y @executeautomation/playwright-mcp-server
```

**Launch Obsidian with debugging:**
```bash
killall Obsidian
open -a Obsidian --args --remote-debugging-port=9222
```

**What MCP enables (98% autonomy):**
- ✅ Keyboard input (`/`, `@`, `Cmd+K`, arrows, Enter, Escape)
- ✅ Console log monitoring (automatic)
- ✅ Screenshot capture (automatic)
- ✅ DOM inspection (automatic)
- ✅ Network request monitoring
- ✅ JavaScript error detection
- ✅ End-to-end test flows

**Typical workflow:**
1. Make plugin changes
2. User reloads plugin (Cmd+R, 2 seconds)
3. MCP validates automatically (keyboard + logs + screenshots + DOM)
4. Iterate until working
5. User reviews (optional)

See `specs/DEVELOPER_EXPERIENCE.md` for comprehensive MCP documentation.

## Philosophy

- **Maximize autonomy** – work independently through all steps when possible
- **Minimize token usage** – be concise, avoid over-verbose explanations
- **Minimal and elegant** over complex and comprehensive
- **Robust implementations** over quick workarounds
- **Respect patterns** that already exist
- **Self-validate** before asking for human review (MCP for plugin, tests for daemon)
- **Test coverage** is a quality metric, maintain or improve it
- **Track progress** with TODO lists to stay organized
