---
description: file formats
alwaysApply: false
---
## Command Files

**Location:** `.spark/commands/*.md`

**Format from PRD:**

```markdown
---
# .spark/commands/summarize.md
id: summarize
name: Summarize Document
description: Create a concise summary of the current document
context: current_file
output: replace_selection
---

Create a **$ARGUMENTS** summary of the provided document. Focus on:
- Key decisions and outcomes
- Action items
- Important dates
- Critical insights

Maintain the document's tone and technical accuracy.
```

**Key fields:**
- `id` - Command identifier
- `name` - Display name in command palette
- `description` - What the command does
- `context` - What context to load (current_file, selection, etc.)
- `output` - Where to put results (replace_selection, append, etc.)

**Body is the prompt sent to AI**, with variables like `$ARGUMENTS`.

---

## Agent Files

**Location:** `.spark/agents/*.md`

**Format from PRD:**

```markdown
---
# .spark/agents/betty.md
name: Betty
role: Senior Accountant & Financial Analyst
expertise:
  - Financial reporting
  - Tax compliance
  - Budget analysis
  - QuickBooks integration
tools:
  - quickbooks
  - stripe
  - excel
context_folders:
  - finance/
  - invoices/
  - reports/financial/
---

You are Betty, a senior accountant with 20 years of experience. You are
meticulous, detail-oriented, and always ensure compliance with regulations.

When analyzing financial data:
1. Always cross-reference multiple sources
2. Flag any discrepancies over $100
3. Provide both summary and detailed views
4. Include relevant tax implications
5. Suggest optimization opportunities
```

**Key fields:**
- `name` - Agent name
- `role` - What they do
- `expertise` - List of areas
- `tools` - MCP services they can use
- `context_folders` - Default folders to load

**Body is the persona/instructions** that get injected into the AI prompt.

---

## SOP Files

**Location:** `.spark/sops/*.md`

**Format from PRD:**

```markdown
---
# .spark/sops/email-status.md
trigger:
  type: frontmatter_change
  field: email_status
  value: send_this_out
context:
  - current_file
  - templates/email/*
---

You are processing an email that needs to be sent. Follow these steps:

1. Extract recipient from frontmatter `to:` field
2. Format the content as professional email using template
3. Send via $gmail integration
4. Update frontmatter:
   - email_status: "sent"
   - sent_date: [timestamp]
5. Move file to sent/[date]/
6. Create follow-up task if deadline mentioned
```

**Key fields:**
- `trigger` - What event triggers this SOP
  - `type` - frontmatter_change, file_created, etc.
  - `field` - Which field to watch
  - `value` - What value triggers it
- `context` - What to load for context

**Body is the automation workflow** - it's the prompt/instructions sent to AI.

---

## Trigger Configuration

**Location:** `.spark/triggers/frontmatter.yaml`

**Format from PRD:**

```yaml
# Frontmatter fields that trigger SOPs
watched_fields:
  - field: email_status
    values: [send_this_out, schedule, draft]
    sop: email-status.md

  - field: task_status
    values: [assigned, in_progress, blocked]
    sop: task-management.md

  - field: invoice_status
    values: [created, sent, overdue]
    sop: invoice-workflow.md

  - field: assigned_to
    pattern: "@*"  # Any @ mention
    sop: task-assigned.md
```

**Structure:**
- `field` - Frontmatter field to watch
- `values` - Specific values that trigger (array)
- `pattern` - Pattern match (alternative to values)
- `sop` - Which SOP file to execute

---

## MCP Integration Configuration

**Location:** `.spark/integrations/<service>/config.yaml`

**Format from PRD:**

```json
{
  "service": "gmail",
  "version": "1.0",
  "authentication": {
    "type": "oauth2",
    "scopes": ["gmail.send", "gmail.readonly"]
  },
  "endpoints": {
    "send": "/gmail/v1/send",
    "fetch": "/gmail/v1/messages",
    "search": "/gmail/v1/search"
  },
  "rate_limits": {
    "requests_per_minute": 60
  },
  "error_handling": {
    "retry_attempts": 3,
    "backoff_multiplier": 2
  }
}
```

**Note:** This is just metadata. Actual MCP servers are installed separately by user.

---

## Main Configuration

**Location:** `.spark/config.yaml`

**Format from PRD:**

```yaml
# Main Spark configuration
version: 1.0
enabled_features:
  slash_commands: true
  assistant: true
  daemon: true

ai_provider:
  default: claude
  fallback: local_llm

interface:
  assistant_hotkey: "Cmd+K"
  command_prefix: "/"

daemon:
  watch_patterns:
    - "*.md"
    - "tasks/**"
    - "invoices/**"
  polling_interval: 1000  # milliseconds

integrations:
  enabled:
    - gmail
    - quickbooks
```

---

## Summary

**All formats follow the same pattern:**
1. YAML frontmatter for metadata
2. Markdown body for instructions/prompts
3. Simple, human-readable
4. Version control friendly

**The body of each file is what gets sent to AI** - commands, agents, and SOPs are all just structured prompts.

**That's it. No complex schemas, no JSON, no XML. Just markdown files.**

---

## What's Actually New?

Looking at what the PRD defines vs what needs to be specced:

**Already defined in PRD:** âœ…
- Command file format
- Agent file format  
- SOP file format
- Trigger config format
- Main config structure

**Still needs speccing:** ðŸ”¨
- Mention parser algorithm (how to parse `@betty review @folder/ and $service`)
- Result writing behavior (where exactly results go)
- Error handling specifics
- Notification JSON schema
- Plugin UI/UX details

**Should we focus on the mention parser next?** That's the most complex logic that isn't already defined in the PRD.
