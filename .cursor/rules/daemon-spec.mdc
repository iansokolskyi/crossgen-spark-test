---
description: daemon specs
alwaysApply: false
---
## File Watching

### What Gets Watched

```typescript
interface WatchConfig {
  paths: string[];              // Default: [vault/**/*.md]
  ignore: string[];             // Default: [.git/, .obsidian/, node_modules/]
  debounce: number;             // Default: 300ms
  batchChanges: boolean;        // Default: true
}
```

### When to Process

**Trigger conditions:**
- ✅ `.md` file saved with Spark syntax
- ✅ Frontmatter changes in watched files
- ✅ Files added/removed in watched folders
- ❌ NOT on every keystroke (wait for save)
- ❌ NOT on auto-save unless syntax present

### Change Detection

```typescript
interface FileChange {
  path: string;
  type: 'created' | 'modified' | 'deleted';
  timestamp: number;
  
  // Parsed diff
  hasSparkSyntax: boolean;
  hasNewCommands: boolean;
  hasFrontmatterChanges: boolean;
  
  // What changed
  newCommands: string[];          // ["/summarize", "@betty review..."]
  frontmatterDiff: {
    added: Record<string, any>;
    modified: Record<string, any>;
    removed: string[];
  };
}
```

---

## Syntax Detection

The daemon looks for these patterns in markdown:

### 1. Slash Commands

```markdown
/summarize
/extract-tasks
/email-draft
```

**Detection regex:** `^\/[\w-]+(\s+.*)?$`

**Characteristics:**
- Must be on its own line
- Starts with `/`
- Followed by command name (alphanumeric, hyphens)
- Optional arguments after space

### 2. At Mentions (Agent/File/Folder)

```markdown
@betty
@finance/Q4/
@compliance-rules.md
```

**Detection regex:** `@([\w-]+(?:\/[\w-]*)*\/?|[\w-]+\.md)`

**Disambiguation rules:**
1. If ends with `/` → Folder
2. If ends with `.md` → File
3. If matches agent name → Agent
4. Else check: agents → files → folders
5. If ambiguous → Prompt or error

### 3. Service References

```markdown
$gmail
$quickbooks
$stripe
```

**Detection regex:** `\$([\w-]+)`

**Must match configured service in:** `.spark/integrations/<service>/config.yaml`

### 4. Tag References

```markdown
#urgent
#client-work
```

**Detection regex:** `#([\w-]+)`

**Loads all files with that tag**

### 5. Complex Lines (Multiple Mentions)

```markdown
@betty review @finance/Q4/ comparing with $quickbooks, flag issues in @compliance.md and /create-report
```

**The daemon:**
1. Parses all tokens in order
2. Loads each mention (agent, files, services)
3. Builds complete context
4. Identifies final command (`/create-report`)
5. Executes with full context

---

## Execution Flow

### Step 1: Parse File

```typescript
interface ParsedFile {
  path: string;
  content: string;
  frontmatter: Record<string, any>;
  
  // Found Spark syntax
  commands: ParsedCommand[];
  mentions: ParsedMention[];
  
  // SOPs triggered by frontmatter
  triggeredSOPs: string[];
}

interface ParsedCommand {
  line: number;
  raw: string;                    // Exact text from file
  type: 'slash' | 'mention-chain';
  
  // For slash commands
  command?: string;               // "summarize"
  args?: string;                  // "in 3 bullet points"
  
  // For mention chains
  mentions?: ParsedMention[];
  finalCommand?: string;
  
  // Execution state
  status: 'pending' | 'processing' | 'completed' | 'error';
}

interface ParsedMention {
  raw: string;                    // "@betty" or "@finance/Q4/"
  type: 'agent' | 'file' | 'folder' | 'service' | 'tag';
  resolved: string;               // Actual path or agent name
  
  // For context loading
  content?: string;               // File content if type=file
  files?: string[];               // File list if type=folder
}
```

### Step 2: Load Context

```typescript
async function loadContext(command: ParsedCommand): Promise<Context> {
  const context: Context = {
    currentFile: command.path,
    mentions: [],
    agentPersona?: null,
    serviceConnections: [],
  };
  
  // Load each mention
  for (const mention of command.mentions) {
    switch (mention.type) {
      case 'agent':
        context.agentPersona = await loadAgent(mention.resolved);
        break;
      
      case 'file':
        const fileContent = await readFile(mention.resolved);
        context.mentions.push({
          type: 'file',
          path: mention.resolved,
          content: fileContent
        });
        break;
      
      case 'folder':
        const files = await readFolder(mention.resolved);
        context.mentions.push({
          type: 'folder',
          path: mention.resolved,
          files: files
        });
        break;
      
      case 'service':
        const service = await connectService(mention.resolved);
        context.serviceConnections.push(service);
        break;
      
      case 'tag':
        const taggedFiles = await findFilesByTag(mention.resolved);
        context.mentions.push({
          type: 'tag',
          tag: mention.resolved,
          files: taggedFiles
        });
        break;
    }
  }
  
  return context;
}
```

### Step 3: Execute Command

```typescript
async function executeCommand(command: ParsedCommand, context: Context): Promise<Result> {
  // Build prompt for AI
  const prompt = buildPrompt({
    command: command,
    context: context,
    agentPersona: context.agentPersona
  });
  
  // Call AI (Claude)
  const response = await ai.complete(prompt);
  
  return {
    status: 'completed',
    output: response,
    timestamp: Date.now()
  };
}
```

### Step 4: Write Results

**The daemon modifies the original file:**

**Before:**
```markdown
Some content...

@betty review @finance/Q4/ and /create-report

More content...
```

**After:**
```markdown
Some content...

✅ @betty review @finance/Q4/ and /create-report

**Betty's Analysis:**
I've reviewed all Q4 financial documents. Here's the summary:
- Revenue: $450K
- Expenses: $320K
- Net: $130K

Full report: @reports/q4-analysis.md

More content...
```

**Implementation:**
```typescript
async function writeResult(file: string, command: ParsedCommand, result: Result) {
  // Read file
  const content = await readFile(file);
  const lines = content.split('\n');
  
  // Find command line
  const commandLine = lines[command.line];
  
  // Mark as completed
  lines[command.line] = `✅ ${commandLine}`;
  
  // Insert result after command
  const resultLines = [
    '',
    result.output,
    ''
  ];
  lines.splice(command.line + 1, 0, ...resultLines);
  
  // Write back
  await writeFile(file, lines.join('\n'));
}
```

---

## Status Indicators

### Visual Feedback in Files

The daemon modifies the command line to show status:

**Pending (just typed):**
```markdown
@betty review @finance/Q4/
```

**Processing (daemon working on it):**
```markdown
⏳ @betty review @finance/Q4/
```

**Completed successfully:**
```markdown
✅ @betty review @finance/Q4/
```

**Failed with error:**
```markdown
❌ @betty review @finance/Q4/
```

**Partially completed (with warnings):**
```markdown
⚠️ @betty review @finance/Q4/
```

### Status Updates

The daemon updates status in real-time:

1. Detects command → Add `⏳`
2. Begins processing → Keep `⏳`
3. On completion → Replace with `✅`
4. On error → Replace with `❌`

**Implementation:**
```typescript
async function updateStatus(file: string, line: number, status: Status) {
  const content = await readFile(file);
  const lines = content.split('\n');
  
  // Remove existing status emoji
  const commandLine = lines[line].replace(/^[✅❌⏳⚠️]\s*/, '');
  
  // Add new status
  const emoji = {
    pending: '',
    processing: '⏳',
    completed: '✅',
    error: '❌',
    warning: '⚠️'
  }[status];
  
  lines[line] = emoji ? `${emoji} ${commandLine}` : commandLine;
  
  await writeFile(file, lines.join('\n'));
}
```

---

## SOP (Standard Operating Procedure) System

### SOP Triggers

SOPs are triggered by frontmatter changes:

```markdown
---
email_status: send_this_out
assigned_to: @betty
task_status: completed
---

Email content here...
```

**Trigger configuration:** `.spark/triggers/frontmatter.yaml`

```yaml
triggers:
  - field: email_status
    values: [send_this_out, schedule]
    sop: email-automation.md
  
  - field: assigned_to
    pattern: "@*"
    sop: task-assignment.md
  
  - field: task_status
    values: [completed]
    sop: task-completion.md
```

### SOP Execution

**When frontmatter changes:**

1. Daemon detects change
2. Looks up matching SOPs
3. Loads SOP file from `.spark/sops/`
4. Executes SOP with file as context
5. SOP may modify file, create new files, call services

**SOP file format:**

```markdown
---
# .spark/sops/email-automation.md
name: Email Automation
trigger:
  type: frontmatter
  field: email_status
  value: send_this_out
context:
  - current_file
  - templates/email/*
---

You are processing an email that needs to be sent.

**Context:**
- Current file contains the email content
- Frontmatter has `to:` field with recipient
- Email templates are available for reference

**Steps:**
1. Extract recipient from `to:` field
2. Format content as professional email
3. Send via $gmail integration
4. Update frontmatter: email_status = "sent"
5. Add sent_date timestamp
6. Move file to sent/[date]/
```

**The SOP is the prompt sent to AI** - it executes the automation.

---

## Notification System

### Notification File Format

`.spark/notifications.jsonl` (JSON Lines - append only)

```jsonl
{"id": "abc123", "type": "success", "message": "Summary completed", "file": "notes.md", "line": 42, "timestamp": 1234567890}
{"id": "def456", "type": "error", "message": "Rate limit exceeded", "file": "email.md", "line": 15, "timestamp": 1234567891, "retry_at": 1234571491}
{"id": "ghi789", "type": "info", "message": "Processing 15 files...", "file": "report.md", "progress": 0.45, "timestamp": 1234567892}
```

### Notification Types

```typescript
interface Notification {
  id: string;                     // Unique ID
  type: 'success' | 'error' | 'warning' | 'info';
  message: string;                // User-facing message
  file: string;                   // Which file triggered this
  line?: number;                  // Line number of command
  timestamp: number;
  
  // Optional fields
  progress?: number;              // 0-1 for long operations
  retry_at?: number;              // For recoverable errors
  details?: string;               // Full error message
  action?: {                      // Action button in toast
    label: string;
    command: string;
  };
}
```

### Plugin Integration

The plugin watches `.spark/notifications.jsonl`:

```typescript
// In plugin
watcher.on('change', async (file) => {
  if (file === '.spark/notifications.jsonl') {
    const newNotifications = await readNewLines(file, lastReadPosition);
    
    for (const notif of newNotifications) {
      showToast(notif);
    }
    
    lastReadPosition = getCurrentPosition(file);
  }
});
```

---

## Error Handling

### Recoverable Errors

**Rate limits, network issues:**

```typescript
if (error.type === 'rate_limit') {
  // Schedule retry
  const retryAt = Date.now() + error.retryAfter * 1000;
  
  await updateStatus(file, line, 'processing');
  await notify({
    type: 'info',
    message: `Rate limited. Retrying in ${error.retryAfter}s`,
    retry_at: retryAt
  });
  
  await scheduleRetry(command, retryAt);
}
```

### Non-Recoverable Errors

**Invalid syntax, missing files:**

```typescript
if (error.type === 'syntax_error') {
  await updateStatus(file, line, 'error');
  await notify({
    type: 'error',
    message: error.message,
    details: error.stack,
    action: {
      label: 'View Logs',
      command: 'spark:open-logs'
    }
  });
  
  // Write error to file
  await writeError(file, line, error);
}
```

**Error written to file:**

```markdown
❌ @betty review @invalid-file.md

**Error:** File not found: @invalid-file.md
```

---

## Configuration

### Main Config

`.spark/config.yaml`

```yaml
daemon:
  watch_patterns: ["**/*.md"]
  ignore_patterns: [".git/**", ".obsidian/**", "node_modules/**"]
  debounce_ms: 300
  
  # Status indicators
  status_emojis:
    pending: ""
    processing: "⏳"
    completed: "✅"
    error: "❌"
    warning: "⚠️"
  
  # Results formatting
  results:
    insert_blank_line: true
    max_inline_length: 500  # Move to separate file if longer

ai:
  provider: claude
  model: claude-3-5-sonnet-20241022
  max_tokens: 4096
  temperature: 0.7

integrations:
  enabled:
    - gmail
    - quickbooks
    - stripe

logging:
  level: info
  file: .spark/logs/daemon.log
  max_size: 10mb
  max_files: 5
```

---

## Questions to Resolve

### 1. Result Placement

When daemon writes results, where exactly?

**Option A: Always inline (below command)**
```markdown
✅ @betty analyze this

**Result:**
Analysis here...
```

**Option B: Inline for short, separate file for long**
```markdown
✅ @betty create detailed report

**Result:** See @reports/2024-01-15-report.md
```

**Option C: User preference**
```yaml
results:
  mode: inline | separate | auto
  auto_threshold: 500 chars
```

### 2. Multiple Commands in One File

What happens here?

```markdown
@betty summarize this section

Some content...

@betty extract tasks from above

More content...

/create-report
```

**Should the daemon:**
- Process all commands in parallel?
- Process sequentially (top to bottom)?
- Only process the first one?
- Let user specify with numbering?

### 3. Re-running Commands

User wants to re-run a completed command:

```markdown
✅ @betty summarize this

Old summary here...
```

**Options:**
- Remove ✅ to re-run
- Add "re-run" keyword: `re-run @betty summarize this`
- Delete the result section
- Keep history and append new result

### 4. Concurrent Executions

Two files modified at same time, both have commands.

**Should daemon:**
- Process in parallel (faster, more resource intensive)
- Queue and process sequentially (safer, slower)
- User configurable limit (max 3 parallel)

---

## Next Steps

Once these questions are resolved, we can:
1. Spec out the Mention Parser in detail
2. Design the Agent system
3. Spec the MCP integration layer
4. Create example SOPs

**Which component should we detail next?**
